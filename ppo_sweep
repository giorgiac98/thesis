command:
  - ${env}
  - ${interpreter}
  - train.py
  - model=ppo
  - ${args_no_hyphens}
method: bayes
metric:
  name: eval/episode_reward
  goal: maximize
parameters:
  frames_per_batch:
    distribution: int_uniform
    max: 10000
    min: 200
  num_envs:
    distribution: categorical
    values:
      - 2
      - 4
      - 8
      - 16
  model.policy_spec.entropy_coef:
    distribution: categorical
    values:
      - 0.0
      - 1e-4
  model.actor_net_spec.num_cells:
    distribution: categorical
    values:
      - 8
      - 16
      - 32
      - 64
  model.value_net_spec.num_cells:
    distribution: categorical
    values:
      - 8
      - 16
      - 32
      - 64
  update_rounds:
    distribution: categorical
    values:
      - 4
      - 8
      - 12
      - 16
      - 20
      - 24
  batch_size:
    distribution: categorical
    values:
      - 32
      - 64
      - 128
      - 256
      - 512
  lr:
    distribution: categorical
    values:
      - 0.01
      - 0.005
      - 0.0025
      - 0.001
      - 0.0005
  schedule_lr:
    distribution: categorical
    values:
      - "True"
      - "False"