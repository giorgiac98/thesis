command:
  - ${env}
  - ${interpreter}
  - train.py
  - model=ppo
  - data.params.method=sequential
  - train_iter=100
  - ${args_no_hyphens}
method: random
parameters:
  frames_per_batch:
    distribution: categorical
    values:
      - 9600
      - 8400
      - 7200
      - 6000
      - 4800
      - 3600
      - 2400
      - 1200
  num_envs:
    distribution: categorical
    values:
      - 2
      - 4
      - 8
      - 16
  model.policy_spec.clip_epsilon:
    distribution: categorical
    values:
      - 0.2
      - 0.15
      - 0.1
  model.policy_spec.entropy_coef:
    distribution: categorical
    values:
      - 0.0
      - 1e-4
  model.policy_spec.critic_coef:
    distribution: categorical
    values:
      - 0.25
      - 0.5
      - 0.75
  model.actor_net_spec.num_cells:
    distribution: categorical
    values:
      - 8
      - 16
      - 32
      - 64
  model.value_net_spec.num_cells:
    distribution: categorical
    values:
      - 8
      - 16
      - 32
      - 64
  model.other_spec.gae_spec.gamma:
    distribution: categorical
    values:
      - 0.9
      - 0.95
      - 0.99
  model.other_spec.gae_spec.lmbda:
    distribution: categorical
    values:
      - 0.9
      - 0.95
      - 0.99
  update_rounds:
    distribution: categorical
    values:
      - 100
      - 90
      - 80
      - 70
      - 60
      - 50
      - 40
      - 30
      - 20
      - 10
  batch_size:
    distribution: categorical
    values:
      - 32
      - 64
      - 128
      - 256
      - 512
  actor_lr:
    distribution: categorical
    values:
      - 0.01
      - 0.005
      - 0.0025
      - 0.001
      - 0.0005
  critic_lr:
    distribution: categorical
    values:
      - 0.01
      - 0.005
      - 0.0025
      - 0.001
      - 0.0005
  schedule_lr:
    distribution: categorical
    values:
      - "True"
      - "False"